{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input & Output State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.graph import END, START, StateGraph\n",
    "\n",
    "model = ChatOpenAI(model = \"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict\n",
    "\n",
    "\n",
    "class ChatMessages(TypedDict):\n",
    "    question: str\n",
    "    answer: str\n",
    "    llm_calls: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_model(state: ChatMessages):\n",
    "    question = state[\"question\"]\n",
    "    llm_calls = state.get(\"llm_calls\", 0)\n",
    "    state[\"llm_calls\"] = llm_calls + 1\n",
    "    print(\"LLM_CALLS:\", state[\"llm_calls\"])\n",
    "    response = model.invoke(input=question)\n",
    "    state[\"answer\"] = response.content\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(ChatMessages)\n",
    "\n",
    "workflow.add_edge(START, \"agent\")\n",
    "workflow.add_node(\"agent\", call_model)\n",
    "workflow.add_edge(\"agent\", END)\n",
    "\n",
    "graph = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM_CALLS: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'Whats the highest mountain in the world?',\n",
       " 'answer': 'The highest mountain in the world is Mount Everest, which stands at an elevation of 8,848.86 meters (29,031.7 feet) above sea level. It is located in the Himalayas on the border between Nepal and the Tibet Autonomous Region of China.',\n",
       " 'llm_calls': 1}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In this case we are not interested in the overall output but rather just the answer and the llm_calls was just an example to\n",
    "#show that we dont want it\n",
    "graph.invoke(input={\"question\": \"Whats the highest mountain in the world?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "\n",
    "class InputState(TypedDict):\n",
    "    question: str\n",
    "\n",
    "\n",
    "class PrivateState(TypedDict):\n",
    "    llm_calls: int\n",
    "\n",
    "\n",
    "class OutputState(TypedDict):\n",
    "    answer: str\n",
    "\n",
    "\n",
    "class OverallState(InputState, PrivateState, OutputState):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(OverallState, input=InputState, output=OutputState)\n",
    "\n",
    "workflow.add_edge(START, \"agent\")\n",
    "workflow.add_node(\"agent\", call_model)\n",
    "workflow.add_edge(\"agent\", END)\n",
    "\n",
    "graph = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM_CALLS: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'answer': 'The highest mountain in the world is Mount Everest, which stands at an elevation of 8,848.86 meters (29,031.7 feet) above sea level. It is located in the Himalayas on the border between Nepal and the Tibet Autonomous Region of China.'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This way we got rid of the extra keys that we dont need\n",
    "graph.invoke({\"question\": \"Whats the highest mountain in the world?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add runtime configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain_core.runnables.config import RunnableConfig\n",
    "from langchain.schema import SystemMessage, HumanMessage\n",
    "\n",
    "\n",
    "def call_model(state: OverallState, config: RunnableConfig):\n",
    "    language = config[\"configurable\"].get(\"language\", \"English\") #Any config must be ran inside \"configuration\" key\n",
    "    system_message_content = \"Respond in {}\".format(language)\n",
    "    system_message = SystemMessage(content=system_message_content)\n",
    "    messages = [system_message, HumanMessage(content=state[\"question\"])]\n",
    "    response = model.invoke(messages)\n",
    "    return {\"answer\": response}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(ChatMessages)\n",
    "\n",
    "workflow.add_edge(START, \"agent\")\n",
    "workflow.add_node(\"agent\", call_model)\n",
    "workflow.add_edge(\"agent\", END)\n",
    "\n",
    "graph = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"language\": \"Spanish\"}}\n",
    "graph.invoke({\"question\": \"What's the highest mountain in the world?\"}, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"language\": \"German\"}}\n",
    "graph.invoke({\"question\": \"What's the highest mountain in the world?\"}, config=config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VE_lang",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
