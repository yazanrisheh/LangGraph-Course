{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1st Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Literal, Annotated, List\n",
    "from operator import add\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, BaseMessage\n",
    "from langgraph.graph import END, START, StateGraph\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "\n",
    "class InputState(TypedDict):\n",
    "    article: str\n",
    "\n",
    "\n",
    "class OutputState(TypedDict):\n",
    "    agent_output: str\n",
    "\n",
    "\n",
    "class OverallState(InputState, OutputState):\n",
    "    messages: Annotated[List[BaseMessage], add]\n",
    "\n",
    "\n",
    "@tool\n",
    "def get_current_club(player_name: str):\n",
    "    \"\"\"Gets current club of a player\"\"\"\n",
    "    fake_db = {\n",
    "        \"Lionel Messi\": \"Paris Saint-Germain\",\n",
    "        \"Cristiano Ronaldo\": \"Al Nassr FC\",\n",
    "    }\n",
    "    return fake_db.get(player_name, \"Current club information not available.\")\n",
    "\n",
    "\n",
    "tools1 = [get_current_club]\n",
    "model1 = ChatOpenAI(model=\"gpt-4o-mini\").bind_tools(tools1)\n",
    "\n",
    "\n",
    "def call_model_current_club(state: OverallState):\n",
    "    # Code below tries to extract msgs key and if theres none then creates empty list to save all msgs\n",
    "    local_messages = state.get(\"messages\", [])\n",
    "    if not local_messages:\n",
    "        human_message = HumanMessage(content=state[\"article\"])\n",
    "        local_messages.append(human_message)\n",
    "    system_message = SystemMessage(\n",
    "        content=\"\"\"You are an agent tasked with determining the current club of a player.\n",
    "If the current club is mentioned, return it. Otherwise, return 'Current club information not available.'\"\"\"\n",
    "    )\n",
    "    response = model1.invoke([system_message] + local_messages)\n",
    "    state[\"agent_output\"] = response.content\n",
    "    state[\"messages\"] = local_messages + [response]\n",
    "    return state\n",
    "\n",
    "\n",
    "def should_continue(state: OverallState) -> Literal[\"tools\", END]:\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    if getattr(last_message, \"tool_calls\", None):\n",
    "        return \"tools\"\n",
    "    return END\n",
    "\n",
    "\n",
    "current_club_graph = StateGraph(OverallState, input=InputState, output=OutputState)\n",
    "current_club_graph.add_node(\"call_model_current_club\", call_model_current_club)\n",
    "current_club_graph.add_node(\"tools\", ToolNode(tools1))\n",
    "current_club_graph.add_edge(START, \"call_model_current_club\")\n",
    "current_club_graph.add_conditional_edges(\"call_model_current_club\", should_continue)\n",
    "current_club_graph.add_edge(\"tools\", \"call_model_current_club\")\n",
    "\n",
    "current_club_researcher_agent = current_club_graph.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'agent_output': \"Lionel Messi's current club is Paris Saint-Germain.\"}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_state = {\n",
    "    \"article\": \"Lionel Messi will join Real Madrid 2025\",\n",
    "}\n",
    "current_club_researcher_agent.invoke(initial_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_state = {\n",
    "    \"article\": \"Thomas Müller will join Real Madrid 2025\",\n",
    "}\n",
    "current_club_researcher_agent.invoke(initial_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2nd Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputState(TypedDict):\n",
    "    article: str\n",
    "\n",
    "\n",
    "class OutputState(TypedDict):\n",
    "    agent_output: str\n",
    "\n",
    "\n",
    "class OverallState(InputState, OutputState):\n",
    "    messages: Annotated[List[BaseMessage], add]\n",
    "\n",
    "\n",
    "@tool\n",
    "def get_market_value(player_name: str):\n",
    "    \"\"\"Gets current market value of a player\"\"\"\n",
    "    fake_market_value_db = {\n",
    "        \"Lionel Messi\": \"€50 million\",\n",
    "        \"Cristiano Ronaldo\": \"€30 million\",\n",
    "    }\n",
    "    return fake_market_value_db.get(\n",
    "        player_name, \"Market value information not available.\"\n",
    "    )\n",
    "\n",
    "\n",
    "tools2 = [get_market_value]\n",
    "model2 = ChatOpenAI(model=\"gpt-4o-mini\").bind_tools(tools2)\n",
    "\n",
    "\n",
    "def call_model_market_value(state: OverallState):\n",
    "    local_messages = state.get(\"messages\", [])\n",
    "    if not local_messages:\n",
    "        human_message = HumanMessage(content=state[\"article\"])\n",
    "        local_messages.append(human_message)\n",
    "    system_message = SystemMessage(\n",
    "        content=\"\"\"You are an agent tasked with determining the market value of a player.\n",
    "If the market value is mentioned, return it. Otherwise, return 'Market value information not available.'\"\"\"\n",
    "    )\n",
    "    response = model2.invoke([system_message] + local_messages)\n",
    "    state[\"agent_output\"] = response.content\n",
    "    state[\"messages\"] = local_messages + [response]\n",
    "    return state\n",
    "\n",
    "\n",
    "def should_continue(state: OverallState) -> Literal[\"tools\", END]:\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    if getattr(last_message, \"tool_calls\", None):\n",
    "        return \"tools\"\n",
    "    return END\n",
    "\n",
    "\n",
    "market_value_graph = StateGraph(OverallState, input=InputState, output=OutputState)\n",
    "market_value_graph.add_node(\"call_model_market_value\", call_model_market_value)\n",
    "market_value_graph.add_node(\"tools\", ToolNode(tools2))\n",
    "market_value_graph.add_edge(START, \"call_model_market_value\")\n",
    "market_value_graph.add_conditional_edges(\"call_model_market_value\", should_continue)\n",
    "market_value_graph.add_edge(\"tools\", \"call_model_market_value\")\n",
    "\n",
    "market_value_researcher_agent = market_value_graph.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'agent_output': 'The market value of Lionel Messi is €50 million.'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "market_value_researcher_agent.invoke(\n",
    "    {\"article\": \"Lionel Messi will switch from FC Barcelona to Real Madrid in 2025\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'agent_output': 'Market value information not available.'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "market_value_researcher_agent.invoke(\n",
    "    {\"article\": \"Thomas Müller will join Real Madrid 2025\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3rd Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputState(TypedDict):\n",
    "    article: str\n",
    "\n",
    "\n",
    "class OutputState(TypedDict):\n",
    "    agent_output: str\n",
    "\n",
    "\n",
    "class OverallState(InputState, OutputState):\n",
    "    pass\n",
    "\n",
    "\n",
    "model_text_writer = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "\n",
    "def expand_text_to_100_words(state: OverallState):\n",
    "    human_message = HumanMessage(content=state[\"article\"])\n",
    "    system_message = SystemMessage(\n",
    "        content=\"Expand the following text to be at least 100 words. Maintain the original meaning while adding detail.\"\n",
    "    )\n",
    "    response = model_text_writer.invoke([system_message, human_message])\n",
    "    state[\"agent_output\"] = response.content\n",
    "    return state\n",
    "\n",
    "\n",
    "text_writer_graph = StateGraph(OverallState, input=InputState, output=OutputState)\n",
    "text_writer_graph.add_node(\"expand_text_to_100_words\", expand_text_to_100_words)\n",
    "text_writer_graph.add_edge(START, \"expand_text_to_100_words\")\n",
    "text_writer_graph.add_edge(\"expand_text_to_100_words\", END)\n",
    "\n",
    "text_writer_agent = text_writer_graph.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_writer_agent.invoke(\n",
    "    {\"article\": \"Lionel Messi will switch from FC Barcelona to Real Madrid in 2025\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervisor Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class ArticlePostabilityGrader(BaseModel):\n",
    "    \"\"\"Binary scores for verifying if an article mentions market value, current club, and meets the minimum word count of 100 words.\"\"\"\n",
    "\n",
    "    off_or_ontopic: str = Field(\n",
    "        description=\"The Article is about football transfers, 'yes' or 'no'\"\n",
    "    )\n",
    "    mentions_market_value: str = Field(\n",
    "        description=\"The article mentions the player's market value, 'yes' or 'no'\"\n",
    "    )\n",
    "    mentions_current_club: str = Field(\n",
    "        description=\"The article mentions the player's current club, 'yes' or 'no'\"\n",
    "    )\n",
    "    meets_100_words: str = Field(\n",
    "        description=\"The article has at least 100 words, 'yes' or 'no'\"\n",
    "    )\n",
    "\n",
    "\n",
    "llm_postability = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "structured_llm_postability_grader = llm_postability.with_structured_output(\n",
    "    ArticlePostabilityGrader\n",
    ")\n",
    "\n",
    "postability_system = \"\"\"\n",
    "You are a grader assessing whether a news article meets the following criteria:\n",
    "1. The article is about football transfers or not. If yes answer, answer with 'yes', anotherwise with 'no'.\n",
    "1. The article explicitly mentions the player's market value, for example, by stating \"market value\" or a specific currency amount (e.g., \"$50 million\"). If this is present, respond with 'yes' for mentions_market_value; otherwise, respond 'no'.\n",
    "2. The article mentions the player's current club or indicates that the current club information is unavailable (e.g., \"Current club information not available\"). If this is present, respond with 'yes' for mentions_current_club; otherwise, respond 'no'.\n",
    "3. The article contains at least 100 words. If this is met, respond with 'yes' for meets_100_words; otherwise, respond 'no'.\n",
    "\n",
    "Provide four binary scores ('yes' or 'no') as follows:\n",
    "- off_or_ontopic: 'yes' or 'no' depending on whether the article is related to football transfers or not.\n",
    "- mentions_market_value: 'yes' or 'no' depending on whether the article mentions the player's market value.\n",
    "- mentions_current_club: 'yes' or 'no' depending on whether the article mentions the player's current club or states that the information is unavailable.\n",
    "- meets_100_words: 'yes' or 'no' depending on whether the article has at least 100 words.\n",
    "\"\"\"\n",
    "\n",
    "postability_grade_prompt = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", postability_system), (\"human\", \"News Article:\\n\\n{article}\")]\n",
    ")\n",
    "\n",
    "news_chef = postability_grade_prompt | structured_llm_postability_grader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_chef.invoke({\"article\": \"Lionel Messi will switch to Real Madrid in 2025\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_chef.invoke({\"article\": \"Today in Munich will be 9°C\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Literal\n",
    "from langgraph.graph import END, StateGraph\n",
    "\n",
    "\n",
    "class InputArticleState(TypedDict):\n",
    "    article: str\n",
    "\n",
    "\n",
    "class OutputFinalArticleState(TypedDict):\n",
    "    final_article: str\n",
    "    off_or_ontopic: str\n",
    "\n",
    "\n",
    "class SharedArticleState(InputArticleState, OutputFinalArticleState):\n",
    "    mentions_market_value: str\n",
    "    mentions_current_club: str\n",
    "    meets_100_words: str\n",
    "\n",
    "\n",
    "def update_article_state(state: SharedArticleState) -> SharedArticleState:\n",
    "    response = news_chef.invoke({\"article\": state[\"article\"]})\n",
    "    state[\"off_or_ontopic\"] = response.off_or_ontopic\n",
    "    state[\"mentions_market_value\"] = response.mentions_market_value\n",
    "    state[\"mentions_current_club\"] = response.mentions_current_club\n",
    "    state[\"meets_100_words\"] = response.meets_100_words\n",
    "    print(\"State after update_article_state:\", state)\n",
    "    return state\n",
    "\n",
    "\n",
    "def market_value_researcher_node(state: SharedArticleState) -> SharedArticleState:\n",
    "    response = market_value_researcher_agent.invoke({\"article\": state[\"article\"]})\n",
    "    state[\"article\"] += f\" {response['agent_output']}\"\n",
    "    return state\n",
    "\n",
    "\n",
    "def current_club_researcher_node(state: SharedArticleState) -> SharedArticleState:\n",
    "    response = current_club_researcher_agent.invoke({\"article\": state[\"article\"]})\n",
    "    state[\"article\"] += f\" {response['agent_output']}\"\n",
    "    return state\n",
    "\n",
    "\n",
    "def word_count_rewriter_node(state: SharedArticleState) -> SharedArticleState:\n",
    "    response = text_writer_agent.invoke({\"article\": state[\"article\"]})\n",
    "    state[\"article\"] += f\" {response['agent_output']}\"\n",
    "    state[\"final_article\"] = response[\"agent_output\"]\n",
    "    return state\n",
    "\n",
    "\n",
    "def news_chef_decider(\n",
    "    state: SharedArticleState,\n",
    ") -> Literal[\n",
    "    \"market_value_researcher\", \"current_club_researcher\", \"word_count_rewriter\", END\n",
    "]:\n",
    "    if state[\"off_or_ontopic\"] == \"no\":\n",
    "        return END\n",
    "    if state[\"mentions_market_value\"] == \"no\":\n",
    "        return \"market_value_researcher\"\n",
    "    elif state[\"mentions_current_club\"] == \"no\":\n",
    "        return \"current_club_researcher\"\n",
    "    elif (\n",
    "        state[\"meets_100_words\"] == \"no\"\n",
    "        and state[\"mentions_market_value\"] == \"yes\"\n",
    "        and state[\"mentions_current_club\"] == \"yes\"\n",
    "    ):\n",
    "        return \"word_count_rewriter\"\n",
    "    else:\n",
    "        return END\n",
    "\n",
    "\n",
    "workflow = StateGraph(\n",
    "    SharedArticleState, input=InputArticleState, output=OutputFinalArticleState\n",
    ")\n",
    "\n",
    "workflow.add_node(\"news_chef\", update_article_state)\n",
    "workflow.add_node(\"market_value_researcher\", market_value_researcher_node)\n",
    "workflow.add_node(\"current_club_researcher\", current_club_researcher_node)\n",
    "workflow.add_node(\"word_count_rewriter\", word_count_rewriter_node)\n",
    "\n",
    "workflow.set_entry_point(\"news_chef\")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"news_chef\",\n",
    "    news_chef_decider,\n",
    "    {\n",
    "        \"market_value_researcher\": \"market_value_researcher\",\n",
    "        \"current_club_researcher\": \"current_club_researcher\",\n",
    "        \"word_count_rewriter\": \"word_count_rewriter\",\n",
    "        END: END,\n",
    "    },\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"market_value_researcher\", \"news_chef\")\n",
    "workflow.add_edge(\"current_club_researcher\", \"news_chef\")\n",
    "workflow.add_edge(\"word_count_rewriter\", \"news_chef\")\n",
    "\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app.invoke({\"article\": \"Today in Munich will be 9°C\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app.invoke({\"article\": \"Lionel Messi will to Real Madrid in 2025\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Human in the Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "checkpointer = MemorySaver()\n",
    "\n",
    "\n",
    "class InputState(TypedDict):\n",
    "    question: str\n",
    "\n",
    "\n",
    "class IntermediateState(InputState):\n",
    "    off_or_ontopic: str\n",
    "    answer: str\n",
    "\n",
    "\n",
    "class FinalState(IntermediateState):\n",
    "    api_response: str\n",
    "    status_code: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_node(state: InputState) -> InputState:\n",
    "    return state\n",
    "\n",
    "\n",
    "def newsagent_node(state: IntermediateState) -> IntermediateState:\n",
    "    response = app.invoke({\"article\": state[\"question\"]})\n",
    "    state[\"answer\"] = response.get(\n",
    "        \"final_article\", \"Article not relevant for news agency\"\n",
    "    )\n",
    "    state[\"off_or_ontopic\"] = response[\"off_or_ontopic\"]\n",
    "    return state\n",
    "\n",
    "\n",
    "def api_call_node(state: FinalState) -> FinalState:\n",
    "    state[\"status_code\"] = 200\n",
    "    state[\"api_response\"] = f\"API received answer: {state['answer']}\"\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x1f40ecc5f90>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workflow = StateGraph(FinalState, input=InputState, output=FinalState)\n",
    "\n",
    "workflow.add_node(\"newsagent_node\", newsagent_node)\n",
    "workflow.add_node(\"api_call_node\", api_call_node)\n",
    "\n",
    "workflow.set_entry_point(\"newsagent_node\")\n",
    "\n",
    "workflow.add_edge(\"newsagent_node\", \"api_call_node\")\n",
    "workflow.add_edge(\"api_call_node\", END)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_app = workflow.compile(\n",
    "    checkpointer=checkpointer, interrupt_after=[\"newsagent_node\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "from langchain_core.runnables.graph import MermaidDrawMethod\n",
    "\n",
    "display(\n",
    "    Image(\n",
    "        human_app.get_graph().draw_mermaid_png(\n",
    "            draw_method=MermaidDrawMethod.API,\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "config2 = {\"configurable\": {\"thread_id\": \"2\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'app' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mhuman_app\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquestion\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mThe weather will be 9°C in Munich\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubgraphs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[0;32m      3\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Asus\\Documents\\LangGraph Course\\VE_lang\\Lib\\site-packages\\langgraph\\pregel\\__init__.py:1936\u001b[0m, in \u001b[0;36mPregel.invoke\u001b[1;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, **kwargs)\u001b[0m\n\u001b[0;32m   1934\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1935\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m-> 1936\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1937\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1938\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1939\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1940\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1941\u001b[0m \u001b[43m    \u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterrupt_before\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1942\u001b[0m \u001b[43m    \u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterrupt_after\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1943\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdebug\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdebug\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1944\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1945\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   1946\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalues\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[0;32m   1947\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlatest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Asus\\Documents\\LangGraph Course\\VE_lang\\Lib\\site-packages\\langgraph\\pregel\\__init__.py:1656\u001b[0m, in \u001b[0;36mPregel.stream\u001b[1;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\u001b[0m\n\u001b[0;32m   1650\u001b[0m     \u001b[38;5;66;03m# Similarly to Bulk Synchronous Parallel / Pregel model\u001b[39;00m\n\u001b[0;32m   1651\u001b[0m     \u001b[38;5;66;03m# computation proceeds in steps, while there are channel updates\u001b[39;00m\n\u001b[0;32m   1652\u001b[0m     \u001b[38;5;66;03m# channel updates from step N are only visible in step N+1\u001b[39;00m\n\u001b[0;32m   1653\u001b[0m     \u001b[38;5;66;03m# channels are guaranteed to be immutable for the duration of the step,\u001b[39;00m\n\u001b[0;32m   1654\u001b[0m     \u001b[38;5;66;03m# with channel updates applied only at the transition between steps\u001b[39;00m\n\u001b[0;32m   1655\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mtick(input_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_channels):\n\u001b[1;32m-> 1656\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1657\u001b[0m \u001b[43m            \u001b[49m\u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtasks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1658\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1659\u001b[0m \u001b[43m            \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1660\u001b[0m \u001b[43m            \u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mget_waiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1661\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   1662\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[0;32m   1663\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1664\u001b[0m \u001b[38;5;66;03m# emit output\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Asus\\Documents\\LangGraph Course\\VE_lang\\Lib\\site-packages\\langgraph\\pregel\\runner.py:239\u001b[0m, in \u001b[0;36mPregelRunner.tick\u001b[1;34m(self, tasks, reraise, timeout, retry_policy, get_waiter)\u001b[0m\n\u001b[0;32m    237\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    238\u001b[0m \u001b[38;5;66;03m# panic on failure or timeout\u001b[39;00m\n\u001b[1;32m--> 239\u001b[0m \u001b[43m_panic_or_proceed\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    240\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdone_futures\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfutures\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    241\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpanic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    242\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Asus\\Documents\\LangGraph Course\\VE_lang\\Lib\\site-packages\\langgraph\\pregel\\runner.py:539\u001b[0m, in \u001b[0;36m_panic_or_proceed\u001b[1;34m(futs, timeout_exc_cls, panic)\u001b[0m\n\u001b[0;32m    537\u001b[0m         \u001b[38;5;66;03m# raise the exception\u001b[39;00m\n\u001b[0;32m    538\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m panic:\n\u001b[1;32m--> 539\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[0;32m    540\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inflight:\n\u001b[0;32m    541\u001b[0m     \u001b[38;5;66;03m# if we got here means we timed out\u001b[39;00m\n\u001b[0;32m    542\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m inflight:\n\u001b[0;32m    543\u001b[0m         \u001b[38;5;66;03m# cancel all pending tasks\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Asus\\Documents\\LangGraph Course\\VE_lang\\Lib\\site-packages\\langgraph\\pregel\\executor.py:76\u001b[0m, in \u001b[0;36mBackgroundExecutor.done\u001b[1;34m(self, task)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdone\u001b[39m(\u001b[38;5;28mself\u001b[39m, task: concurrent\u001b[38;5;241m.\u001b[39mfutures\u001b[38;5;241m.\u001b[39mFuture) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 76\u001b[0m         \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m GraphBubbleUp:\n\u001b[0;32m     78\u001b[0m         \u001b[38;5;66;03m# This exception is an interruption signal, not an error\u001b[39;00m\n\u001b[0;32m     79\u001b[0m         \u001b[38;5;66;03m# so we don't want to re-raise it on exit\u001b[39;00m\n\u001b[0;32m     80\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtasks\u001b[38;5;241m.\u001b[39mpop(task)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\concurrent\\futures\\_base.py:449\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    447\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m--> 449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    451\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[0;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\concurrent\\futures\\_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[0;32m    400\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 401\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    403\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    404\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\concurrent\\futures\\thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "File \u001b[1;32mc:\\Users\\Asus\\Documents\\LangGraph Course\\VE_lang\\Lib\\site-packages\\langgraph\\pregel\\retry.py:40\u001b[0m, in \u001b[0;36mrun_with_retry\u001b[1;34m(task, retry_policy, configurable)\u001b[0m\n\u001b[0;32m     38\u001b[0m     task\u001b[38;5;241m.\u001b[39mwrites\u001b[38;5;241m.\u001b[39mclear()\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m     42\u001b[0m     ns: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "File \u001b[1;32mc:\\Users\\Asus\\Documents\\LangGraph Course\\VE_lang\\Lib\\site-packages\\langgraph\\utils\\runnable.py:408\u001b[0m, in \u001b[0;36mRunnableSeq.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    404\u001b[0m config \u001b[38;5;241m=\u001b[39m patch_config(\n\u001b[0;32m    405\u001b[0m     config, callbacks\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mget_child(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseq:step:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    406\u001b[0m )\n\u001b[0;32m    407\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 408\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    409\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    410\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[1;32mc:\\Users\\Asus\\Documents\\LangGraph Course\\VE_lang\\Lib\\site-packages\\langgraph\\utils\\runnable.py:184\u001b[0m, in \u001b[0;36mRunnableCallable.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    183\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, config)\n\u001b[1;32m--> 184\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecurse:\n\u001b[0;32m    186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "Cell \u001b[1;32mIn[8], line 6\u001b[0m, in \u001b[0;36mnewsagent_node\u001b[1;34m(state)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnewsagent_node\u001b[39m(state: IntermediateState) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m IntermediateState:\n\u001b[1;32m----> 6\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mapp\u001b[49m\u001b[38;5;241m.\u001b[39minvoke({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marticle\u001b[39m\u001b[38;5;124m\"\u001b[39m: state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m]})\n\u001b[0;32m      7\u001b[0m     state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mget(\n\u001b[0;32m      8\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinal_article\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArticle not relevant for news agency\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      9\u001b[0m     )\n\u001b[0;32m     10\u001b[0m     state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moff_or_ontopic\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moff_or_ontopic\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'app' is not defined",
      "\u001b[0mDuring task with name 'newsagent_node' and id '1e8643f7-8945-f506-68fa-e6d78cf1c3b9'"
     ]
    }
   ],
   "source": [
    "human_app.invoke(\n",
    "    {\"question\": \"The weather will be 9°C in Munich\"}, config=config2, subgraphs=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_app.invoke(\n",
    "    {\"question\": \"Lionel Messi will to Real Madrid in 2025\"},\n",
    "    config=config,\n",
    "    subgraphs=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snapshot = human_app.get_state(config2)\n",
    "snapshot.next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_app.invoke(None, config=config2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snapshot = human_app.get_state(config)\n",
    "snapshot.next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snapshot = human_app.get_state(config)\n",
    "existing_message = snapshot.values\n",
    "existing_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_app.update_state(\n",
    "    config,\n",
    "    {\"answer\": \"Fake news!!!\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_app.invoke(None, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snapshot = human_app.get_state(config)\n",
    "snapshot.next"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VE_lang",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
